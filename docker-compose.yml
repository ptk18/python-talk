services:
  backend:
    build: ./backend
    platform: linux/amd64
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
    networks:
      - python_talk_net
    environment:
      # Disable SentenceTransformer for fast startup
      - DISABLE_SEMANTIC_MATCHING=true
      # Disable Whisper voice model pre-warming (saves 4.5GB RAM)
      # Voice models will lazy-load on first transcription request
      - PREWARM_MODELS=false
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 512M

  main-app:
    build:
      context: .
      dockerfile: main-app/Dockerfile
    platform: linux/amd64
    ports:
      - "3001:3001"
    volumes:
      - ./main-app:/app/main-app
      - ./packages/shared:/app/packages/shared
      - /app/main-app/node_modules
    depends_on:
      - backend
    networks:
      - python_talk_net
    environment:
      - VITE_API_URL=http://backend:8000

  codespace-app:
    build:
      context: .
      dockerfile: codespace-app/Dockerfile
    platform: linux/amd64
    ports:
      - "3002:3002"
    volumes:
      - ./codespace-app:/app/codespace-app
      - ./packages/shared:/app/packages/shared
      - /app/codespace-app/node_modules
    depends_on:
      - backend
    networks:
      - python_talk_net
    environment:
      - VITE_API_URL=http://backend:8000

  turtle-app:
    build:
      context: .
      dockerfile: turtle-app/Dockerfile
    platform: linux/amd64
    ports:
      - "3003:3003"
    volumes:
      - ./turtle-app:/app/turtle-app
      - ./packages/shared:/app/packages/shared
      - /app/turtle-app/node_modules
    depends_on:
      - backend
    networks:
      - python_talk_net
    environment:
      - VITE_API_URL=http://backend:8000

networks:
  python_talk_net:
